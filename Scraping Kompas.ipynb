{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \{\
 "cells": [\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "# Scraping Kompas.com\\n",\
    "\\n",\
    "Pada python notebook ini akan dijelaskan mengenai scraping artikel dalam kompas.com\\n",\
    "\\n",\
    "Scraping dimulai dari halaman awal, lalu akan menelusuri setiap link, dan akan mengambil artikel beserta properties lainnya.\\n",\
    "\\n",\
    "\\n",\
    "### Inisialisasi Url Lib\\n",\
    "\\n",\
    "urllib merupakan modul python untuk mengakses halaman web. Digunakan fungsi urlopen dan read untuk membaca kode HTML. Dapat dilihat r merupakan string HTML."\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [],\
   "source": [\
    "import urllib\\n",\
    "\\n",\
    "r = urllib.urlopen('http://tekno.kompas.com/').read()\\n",\
    "print(type(r))\\n",\
    "print(r[1:1000])\\n"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Inisialisasi BeautifulSoup\\n",\
    "\\n",\
    "Untuk membantu dalam proses web scraping digunakan modul beautifulsoup.\\n",\
    "\\n",\
    "Pertama tama diimport dahulu modulnya, lalu string r diubah menjadi objek beautiful soup dan digunakan parser lxml\\n",\
    "\\n",\
    "Jika di print soup nya, sekilas terlihat seperti string HTML, namun sebenarnya itu merupakan sebuah objek soup yang bisa ditelusri setiap elemennya!"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [],\
   "source": [\
    "from bs4 import BeautifulSoup \\n",\
    "\\n",\
    "soup = BeautifulSoup(r,\\"lxml\\")\\n",\
    "print(type(soup))\\n",\
    "print(soup.prettify()[1:1000])"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Dapatkan Semua Link\\n",\
    "\\n",\
    "Untuk mendapatkan semua link kita dapat mencari semua tag html \\"a\\". Namun tentunya tidak semua hasilnya merupakan link artikel. perlu kita filter terlebih dahulu."\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [],\
   "source": [\
    "links = soup.find_all('a')\\n",\
    "print(len(links))\\n",\
    "\\n",\
    "for link in links:\\n",\
    "    print('%s: %s'%(link.text.strip(),link[\\"href\\"]))\\n"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "Dapat dilihat link yang merujuk ke berita memiliki pola '/read/' dan benar-benar link bukan javascript maupun link '#'. Maka dari itu bisa kita filter, dan hasilnya bisa kita simpan di sebuah list"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [],\
   "source": [\
    "berita_link = [link for link in links if '/read/' in str(link) and 'javascript:void(0)' not in str(link) \\n",\
    "               and '#' not in str(link)]\\n",\
    "print(len(berita_link))\\n",\
    "for link in berita_link:\\n",\
    "    print('%s: %s'%(link.text.strip(),link[\\"href\\"]))\\n"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Dapatkan Isi Berita\\n",\
    "\\n",\
    "Sebelum kita menelusuri satu per satu kita perlu tahu dahulu struktur dari sebuah halaman berita pada kompas.com untuk mendapatkan properties yang kita inginkan. \\n",\
    "\\n",\
    "Gunakan inspect element!\\n",\
    "\\n",\
    "Misalkan disini ingin diambil judul berita, isi berita, beserta tanggal pembuatan berita. Setelah dianalisa, judul berada pada elemen div yang mempunyai class kcm-read-top, sedangkan isi_berita pada div yang mempunyai kelas kcm-read-text, dan tanggal div dengan class kcm-date. Untuk mendapatkan text atau inner html nya kita bisa memanfaatkan properti text.\\n",\
    "\\n",\
    "Namun disini terdapat kendala karena tag tanggal tergabung dengan berbagai text lainnya jadi harus dilakukan pemrosesan terlebih dahulu. Cara yang akan digunakan disini sedikit berbahaya karena memanfaatkan separator yang bisa jadi tidak konsisten di semua berita, meskipun sampai saat ini belum ditemukan."\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [],\
   "source": [\
    "html = urllib.urlopen(\\"http://tekno.kompas.com/read/2016/04/26/19500067/6.Aplikasi.Gratis.Pengirit.Baterai.Android\\").read()\\n",\
    "soup = BeautifulSoup(html, \\"lxml\\")\\n",\
    "\\n",\
    "judul = soup.find(\\"div\\",\\"kcm-read-top\\").find(\\"h2\\").text.strip()\\n",\
    "isi_berita = soup.find(\\"div\\",\\"kcm-read-text\\").text.strip()\\n",\
    "tanggal = soup.find(\\"div\\",\\"kcm-date\\").text\\n",\
    "\\n",\
    "#print(tanggal.strip())\\n",\
    "tanggal = tanggal.strip().split(',')[1].split('|')[0].strip()\\n",\
    "print(tanggal)"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "Sebagai tambahan, python juga dapat menyimpan sebuah gambar. Disini akan coba kita terapkan untuk menyimpan gambar dari berita. "\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [],\
   "source": [\
    "gambar = soup.find(\\"div\\",\\"kcm-read-top\\").find(\\"img\\")\\n",\
    "print(gambar['src'])\\n",\
    "\\n",\
    "from PIL import Image\\n",\
    "from StringIO import StringIO\\n",\
    "import string\\n",\
    "\\n",\
    "r = urllib.urlopen(gambar[\\"src\\"]).read()\\n",\
    "print(r)\\n",\
    "\\n",\
    "i = Image.open(StringIO(r))\\n",\
    "exclude = set(string.punctuation)\\n",\
    "judul = ''.join(ch for ch in judul if ch not in exclude)\\n",\
    "\\n",\
    "nama_file = \\"images/\\" + judul + \\".jpg\\"\\n",\
    "i.save(nama_file,'JPEG')\\n"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Gabungkan ke Dalam Fungsi"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": true\
   \},\
   "outputs": [],\
   "source": [\
    "def getBerita(link):\\n",\
    "    html = urllib.urlopen(link).read()\\n",\
    "    soup = BeautifulSoup(html, \\"lxml\\")\\n",\
    "\\n",\
    "    judul = soup.find(\\"div\\",\\"kcm-read-top\\").find(\\"h2\\").text.strip().encode(\\"utf8\\")\\n",\
    "    isi_berita = soup.find(\\"div\\",\\"kcm-read-text\\").text.strip().encode(\\"utf8\\")\\n",\
    "    tanggal = soup.find(\\"div\\",\\"kcm-date\\").text\\n",\
    "    tanggal = tanggal.strip().split(',')[1].split('|')[0].strip()\\n",\
    "    gambar = soup.find(\\"div\\",\\"kcm-read-top\\").find(\\"img\\")\\n",\
    "    \\n",\
    "    r = urllib.urlopen(gambar[\\"src\\"]).read()\\n",\
    "    i = Image.open(StringIO(r))\\n",\
    "    \\n",\
    "    exclude = set(string.punctuation)\\n",\
    "    judul = ''.join(ch for ch in judul if ch not in exclude)\\n",\
    "    \\n",\
    "    nama_file = \\"images/\\" + judul + \\".jpg\\"\\n",\
    "    i.save(nama_file,'JPEG')\\n",\
    "    \\n",\
    "    return [judul,isi_berita,tanggal]"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Dapatkan Semua Data dari Link Berita"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [],\
   "source": [\
    "berita = []\\n",\
    "for link in berita_link:\\n",\
    "    print('%s: %s'%(link.text.strip(),link[\\"href\\"]))\\n",\
    "    try:\\n",\
    "        berita.append(getBerita(link[\\"href\\"]))\\n",\
    "    except:\\n",\
    "        pass"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Simpan ke dalam CSV"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [],\
   "source": [\
    "#print(berita)\\n",\
    "import csv\\n",\
    "\\n",\
    "with open('data_berita.csv', 'wb') as f:\\n",\
    "    writer = csv.writer(f)\\n",\
    "    writer.writerows(berita)"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": true\
   \},\
   "outputs": [],\
   "source": []\
  \}\
 ],\
 "metadata": \{\
  "kernelspec": \{\
   "display_name": "Python 2",\
   "language": "python",\
   "name": "python2"\
  \},\
  "language_info": \{\
   "codemirror_mode": \{\
    "name": "ipython",\
    "version": 2\
   \},\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython2",\
   "version": "2.7.11"\
  \}\
 \},\
 "nbformat": 4,\
 "nbformat_minor": 0\
\}\
}